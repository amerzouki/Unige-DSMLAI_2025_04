{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "018d76a9",
   "metadata": {},
   "source": [
    "\n",
    "# Building a PDF Question Answering (QA) System using RAG\n",
    "> In this notebook, you will create a system that answers questions based on the content of a PDF file. \n",
    "> This involves building a Retrieval-Augmented Generation (RAG) pipeline, where relevant document sections \n",
    "> are retrieved based on the question and used to generate precise answers.\n",
    "\n",
    "## Key Components:\n",
    "1. **Document Loading** - Load and preprocess PDF content for analysis.\n",
    "2. **Indexing** - Split and store text in a vectorized format for fast retrieval.\n",
    "3. **Retrieval** - Retrieve relevant document sections for a given query.\n",
    "4. **Generation** - Use a language model to generate an answer based on the retrieved context.\n",
    "\n",
    "Let's dive in!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea7bd9e",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“¥ Library Installation\n",
    "We install the necessary libraries to process PDFs, handle embeddings, and work with language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: faiss-cpu in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from faiss-cpu) (2.2.3)\n",
      "Requirement already satisfied: packaging in /Users/azizamerzouki/Library/Python/3.13/lib/python/site-packages (from faiss-cpu) (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-openai in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.3.13)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.52 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-openai) (0.3.52)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-openai) (1.73.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-openai) (0.3.30)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/azizamerzouki/Library/Python/3.13/lib/python/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-openai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-openai) (2.10.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.52->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-openai) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.52->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.52->langchain-openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pypdf langchain_community\n",
    "%pip install faiss-cpu\n",
    "%pip install langchain-openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“¤ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import config  # Importing the config file\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from langchain_community.vectorstores import FAISS # A library for efficient similarity search and clustering of dense vectors.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepration\n",
    "## Loading Documents\n",
    "> First, you'll need to choose a PDF to load. Feel free to use a PDF of your choosing.\n",
    "> Once you've chosen your PDF, the next step is to load it into a format that an LLM can more easily handle, since LLMs generally require text inputs. \n",
    "> LangChain has a few different built-in PDF document loaders for this purpose which you can experiment with. \n",
    "\n",
    "> Below, we'll use one powered by the <a href=\"https://pypi.org/project/pypdf/\">pypdf</a> package that reads from a filepath.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Documents\n",
    "\n",
    "# Example document\n",
    "file_path = \"./example_data/journal.pone.0264429.pdf\"\n",
    "\n",
    "# The loader reads the PDF at the specified path into memory.\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "# Extract text data using the pypdf package.\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Type and length of docs\n",
    "print(type(docs))\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What do each element of the variable docs represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of docs elements: \n",
      "<class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "First element of loaded data docs: \n",
      "page_content='RESEA RCH ARTICL E\n",
      "Prediction of HIV status based on socio-\n",
      "behavioural characteristics in East and\n",
      "Southern Africa\n",
      "Erol Orel\n",
      "ID\n",
      "1\n",
      "*, Rachel Esra\n",
      "1\n",
      ", Janne Estill\n",
      "1,2\n",
      ", Amaury Thiabaud\n",
      "ID\n",
      "1\n",
      ", Ste Â´ phane Marchand-\n",
      "Maillet\n",
      "ID\n",
      "3\n",
      ", Aziza Merzouki\n",
      "1â˜¯\n",
      ", Olivia Keiser\n",
      "1â˜¯\n",
      "1 Institute of Global Health, University of Geneva, Geneva , Switzerlan d, 2 Institute of Mathematic al Statistics\n",
      "and Actuari al Science, Univers ity of Bern, Bern, Switzerlan d, 3 Department of Computer Science, Viper\n",
      "Group, University of Geneva, Geneva , Switzerlan d\n",
      "â˜¯ These authors contribu ted equally to this work.\n",
      "* Erol.Ore l@unige.ch\n",
      "Abstract\n",
      "Introduction\n",
      "High yield HIV testing strategies are critical to reach epidemic control in high prevalence and\n",
      "low-resource settings such as East and Southern Africa. In this study, we aimed to predict\n",
      "the HIV status of individuals living in Angola, Burundi, Ethiopia, Lesotho, Malawi, Mozam-\n",
      "bique, Namibia, Rwanda, Zambia and Zimbabwe with the highest precision and sensitivity\n",
      "for different policy targets and constraints based on a minimal set of socio-behavioural\n",
      "characteristics.\n",
      "Methods\n",
      "We analysed the most recent Demographic and Health Survey from these 10 countries to\n",
      "predict individualâ€™s HIV status using four different algorithms (a penalized logistic regression,\n",
      "a generalized additive model, a support vector machine, and a gradient boosting trees). The\n",
      "algorithms were trained and validated on 80% of the data, and tested on the remaining 20%.\n",
      "We compared the predictions based on the F1 score, the harmonic mean of sensitivity and\n",
      "positive predictive value (PPV), and we assessed the generalization of our models by test-\n",
      "ing them against an independent left-out country. The best performing algorithm was trained\n",
      "on a minimal subset of variables which were identified as the most predictive, and used to 1)\n",
      "identify 95% of people living with HIV (PLHIV) while maximising precision and 2) identify\n",
      "groups of individuals by adjusting the probability threshold of being HIV positive (90% in our\n",
      "scenario) for achieving specific testing strategies.\n",
      "Results\n",
      "Overall 55,151 males and 69,626 females were included in the analysis. The gradient boost-\n",
      "ing trees algorithm performed best in predicting HIV status with a mean F1 score of 76.8%\n",
      "[95% confidence interval (CI) 76.0%-77.6%] for males (vs [CI 67.8%-70.6%] for SVM) and\n",
      "78.8% [CI 78.2%-79.4%] for females (vs [CI 73.4%-75.8%] for SVM). Among the ten most\n",
      "PLOS ONE\n",
      "PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02644 29 March 3, 2022 1 / 15\n",
      "a1111111111\n",
      "a1111111111\n",
      "a1111111111\n",
      "a1111111111\n",
      "a1111111111\n",
      "OPEN ACCESS\n",
      "Citation: Orel E, Esra R, Estill J, Thiabaud A,\n",
      "Marchand-M aillet S, Merzouki A, et al. (2022)\n",
      "Prediction of HIV status based on socio-\n",
      "behavioural characteristi cs in East and Southern\n",
      "Africa. PLoS ONE 17(3): e0264429. https:// doi.org/\n",
      "10.1371/ journal.pone. 0264429\n",
      "Editor: Daniel Boateng, Julius Centre, University\n",
      "Medical Centre, Utrecht, NETHERLA NDS\n",
      "Received: March 29, 2021\n",
      "Accepted: February 10, 2022\n",
      "Published: March 3, 2022\n",
      "Peer Review History: PLOS recognize s the\n",
      "benefits of transpar ency in the peer review\n",
      "process; therefore, we enable the publication of\n",
      "all of the content of peer review and author\n",
      "response s alongside final, published articles. The\n",
      "editorial history of this article is available here:\n",
      "https://doi.o rg/10.1371/jo urnal.pone.0 264429\n",
      "Copyright: Â© 2022 Orel et al. This is an open\n",
      "access article distributed under the terms of the\n",
      "Creative Commons Attribution License, which\n",
      "permits unrestricte d use, distribu tion, and\n",
      "reproduction in any medium, provided the original\n",
      "author and source are credited.\n",
      "Data Availabilit y Statement: The data are to be\n",
      "found on the DHS website: the data are held in a\n",
      "public repository: https://dh sprogram.com /data/\n",
      "available-data sets.cfm.' metadata={'producer': 'PDFlib+PDI 9.3.1p2 (C++/Win64)', 'creator': 'PTC Arbortext Layout Developer 12.1.6180/W-x64', 'creationdate': '2022-02-25T11:26:59+05:30', 'title': 'Prediction of HIV status based on socio-behavioural characteristics in East and Southern Africa', 'epsprocessor': 'PStill version 1.84.42', 'author': 'Erol Orel, Rachel Esra, Janne Estill, Amaury Thiabaud, StÃ©phane Marchand-Maillet, Aziza Merzouki, Olivia Keiser', 'moddate': '2022-02-25T11:26:59+05:30', 'source': './example_data/journal.pone.0264429.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Type of docs elements\n",
    "print(f\"Type of docs elements: \\n{type(docs[0])}\")\n",
    "# Display first element of docs\n",
    "print(f\"\\nFirst element of loaded data docs: \\n{docs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So what just happened?**\n",
    "\n",
    "- The loader reads the PDF at the specified path into memory.\n",
    "- It then extracts text data using the pypdf package.\n",
    "- Finally, it creates a LangChain Document for each page of the PDF with the page's content and some metadata about where in the document the text came from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitting\n",
    "We split the document into smaller (more focused) chunks that can more easily fit into an LLM's context window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chuncks: 60\n",
      "\n",
      "Type of chuncks: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Content of first text chunck:\n",
      "RESEA RCH ARTICL E\n",
      "Prediction of HIV status based on socio-\n",
      "behavioural characteristics in East and\n",
      "Southern Africa\n",
      "Erol Orel\n",
      "ID\n",
      "1\n",
      "*, Rachel Esra\n",
      "1\n",
      ", Janne Estill\n",
      "1,2\n",
      ", Amaury Thiabaud\n",
      "ID\n",
      "1\n",
      ", Ste Â´ phane Marchand-\n",
      "Maillet\n",
      "ID\n",
      "3\n",
      ", Aziza Merzouki\n",
      "1â˜¯\n",
      ", Olivia Keiser\n",
      "1â˜¯\n",
      "1 Institute of Global Health, University of Geneva, Geneva , Switzerlan d, 2 Institute of Mathematic al Statistics\n",
      "and Actuari al Science, Univers ity of Bern, Bern, Switzerlan d, 3 Department of Computer Science, Viper\n",
      "Group, University of Geneva, Geneva , Switzerlan d\n",
      "â˜¯ These authors contribu ted equally to this work.\n",
      "* Erol.Ore l@unige.ch\n",
      "Abstract\n",
      "Introduction\n",
      "High yield HIV testing strategies are critical to reach epidemic control in high prevalence and\n",
      "low-resource settings such as East and Southern Africa. In this study, we aimed to predict\n",
      "the HIV status of individuals living in Angola, Burundi, Ethiopia, Lesotho, Malawi, Mozam-\n",
      "bique, Namibia, Rwanda, Zambia and Zimbabwe with the highest precision and sensitivity\n"
     ]
    }
   ],
   "source": [
    "# Explore results of text splitting into chunks\n",
    "print(f\"Number of chuncks: {len(documents)}\\n\")\n",
    "print(f\"Type of chuncks: {type(documents[0])}\\n\")\n",
    "print(f\"Content of first text chunck:\\n{documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question**: Try changing the chunk sizes! \n",
    "- How does it affect the text splitting result?\n",
    "- How does it affect the generated answers below?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "## Embedding and Vector Storage\n",
    "Once we have split text chunks, we store them as vectors (embeddings) into a Vector Store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (228386707.py, line 7)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mvectorstore= InMemoryVectorStore.from_documents(\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = config.OPENAI_API_KEY\n",
    "\n",
    " vectorstore= InMemoryVectorStore.from_documents(\n",
    "    documents=documents, embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "We set up a retriever from the vector store to identify relevant document chunks based on a user's query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "#retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retriever allows us to perform similarity search, and retrieve relevant text chunks based on query embeddings.\n",
    "\n",
    "This involves comparing query embeddings with stored text embeddings to retrieve the most relevant passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query examples\n",
    "#query = \"What is the goal of the study?\"\n",
    "query = \"What is 95-95-95?\" #Check retrieved text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieved text chuncks: 4\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Number of retrieved text chuncks: {len(retrieved_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the default number of retrieved text chunks?\n",
    "- Change the number of retrieved chunks, e.g. 1. See [Documentation](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/vectorstore/#specifying-top-k)\n",
    "- See effect on LLM answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Generation with OpenAI\n",
    "Here, we use the retrieved document chunks to answer the user query through an OpenAI language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = config.OPENAI_API_KEY\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation (Context Integration) Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrieved text provides context, enabling the model to generate accurate and contextually relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Keep the \"\n",
    "    \"answer precise and concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is 95-95-95?\n",
      "\n",
      "Answer: The 95-95-95 initiative is a global goal aimed at ensuring that 95% of people living with HIV know their status, 95% of those diagnosed with HIV receive sustained antiretroviral therapy, and 95% of those on treatment achieve viral suppression. This strategy seeks to end the HIV epidemic by improving detection, treatment, and health outcomes for individuals with HIV.\n",
      "\n",
      "Sources:\n",
      "\n",
      " page_content='ity) with the highest possible yield (positive predictive value (PPV)). We, therefore, used the F1\n",
      "score for assessing the performance of the different algorithms. This metric combines the sen-\n",
      "sitivity and the precision in a harmonic mean and is often recommended for unbalanced data-\n",
      "sets when comparing models [26]. The probability threshold to classify if someone is\n",
      "considered HIV positive was set at 50%. In addition, to validate our results with a strictly\n",
      "proper scoring rule, we also computed the Brier score. This score is strictly equivalent to the\n",
      "mean squared error as applied to predicted probabilities for unidimensional predictions.\n",
      "The analysis was done in two steps for each of the four algorithms (Fig 1â€”Step 2, 3), and\n",
      "separately for males and females. Training and validation were performed using the stratified\n",
      "5-fold cross-validation on the training sample with 50 different sets of hyperparameters ran-' metadata={'source': './example_data/journal.pone.0264429.pdf', 'page': 3}\n",
      "\n",
      "\n",
      " page_content='Test 605 278 129 10,019 74.8% 68.5% 82.4%\n",
      "27 variables with XGBoost imputatio n (Model F3) Validation 75ï¿½6% (Â± 1ï¿½2%) 70ï¿½0% (Â± 1ï¿½2%) 82ï¿½2% (Â± 1ï¿½7%)\n",
      "Test 1,212 390 234 12,090 79.5% 75.7% 83.8%\n",
      "9 variables with XGBoost imputati on (Model M4) Validation 72ï¿½9% (Â± 2ï¿½3%) 65ï¿½6% (Â± 1ï¿½6%) 81ï¿½9% (Â± 3ï¿½9%)\n",
      "Test 595 288 124 10,024 74.3% 67.4% 82.8%\n",
      "9 variables with XGBoost imputati on (Model F4) Validation 72ï¿½4% (Â± 1ï¿½2%) 68ï¿½5% (Â± 1ï¿½4%) 76ï¿½8% (Â± 1ï¿½6%)\n",
      "Test 1,184 418 249 12,075 78.0% 73.9% 82.6%\n",
      "True Positive (TP), False Negativ e (FN), False Positive (FP), True Negativ e (TN), Positive Predictive Value (PPV)\n",
      "Multiple Imputation by Chained Equations (MICE).\n",
      "(Â± %): 95% Confidence Interval.\n",
      "https://do i.org/10.1371/j ournal.pone .0264429.t001\n",
      "PLOS ONE\n",
      "Prediction of HIV status based on socio-beh avioural character istics in East and Southe rn Africa\n",
      "PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02644 29 March 3, 2022 7 / 15' metadata={'source': './example_data/journal.pone.0264429.pdf', 'page': 6}\n",
      "\n",
      "\n",
      " page_content='F4), we further analysed the results at country level, comparing the F1, sensitivity and PPV\n",
      "between countries and the differences between observed and predicted prevalences.\n",
      "Scenarios\n",
      "We tested two scenarios: for the first scenario, the sensitivity was set to 95%, equivalent to 95%\n",
      "of PLHIV knowing their status, and we reported the corresponding precision and number of\n",
      "individuals to be tested. For the second scenario, we identified a population for which the\n",
      "probability of being HIV positive was higher than 90%. We considered that these groups of\n",
      "individuals are targets for specific testing strategies or ideal candidates for prevention services.\n",
      "Ethical review\n",
      "No ethical approval was needed for this study.\n",
      "Data and code availability\n",
      "The data supporting the findings of this study are available from the DHS Program https://\n",
      "dhsprogram.com/. The DHS Program is authorized to distribute, at no cost, unrestricted sur-' metadata={'source': './example_data/journal.pone.0264429.pdf', 'page': 4}\n",
      "\n",
      "\n",
      " page_content='78.8% [CI 78.2%-79.4%] for females (vs [CI 73.4%-75.8%] for SVM). Among the ten most\n",
      "PLOS ONE\n",
      "PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02644 29 March 3, 2022 1 / 15\n",
      "a1111111111\n",
      "a1111111111\n",
      "a1111111111\n",
      "a1111111111\n",
      "a1111111111\n",
      "OPEN ACCESS\n",
      "Citation: Orel E, Esra R, Estill J, Thiabaud A,\n",
      "Marchand-M aillet S, Merzouki A, et al. (2022)\n",
      "Prediction of HIV status based on socio-\n",
      "behavioural characteristi cs in East and Southern\n",
      "Africa. PLoS ONE 17(3): e0264429. https:// doi.org/\n",
      "10.1371/ journal.pone. 0264429\n",
      "Editor: Daniel Boateng, Julius Centre, University\n",
      "Medical Centre, Utrecht, NETHERLA NDS\n",
      "Received: March 29, 2021\n",
      "Accepted: February 10, 2022\n",
      "Published: March 3, 2022\n",
      "Peer Review History: PLOS recognize s the\n",
      "benefits of transpar ency in the peer review\n",
      "process; therefore, we enable the publication of\n",
      "all of the content of peer review and author\n",
      "response s alongside final, published articles. The\n",
      "editorial history of this article is available here:' metadata={'source': './example_data/journal.pone.0264429.pdf', 'page': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a chain for passing a list of Documents to a model.\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "# Create retrieval chain that retrieves documents and then passes them on.\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Answer: {results['answer']}\\n\")\n",
    "print(\"Sources:\")\n",
    "for document in results[\"context\"]:\n",
    "    print(f\"\\n {document}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Change the number of retrieved text chunks used as context and play with the augmented prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ Summary\n",
    "In this notebook, you built a PDF ingestion and question-answering system using Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "You covered the following aspects:\n",
    "> - Loading and processing a PDF file into manageable text chunks\n",
    "> - Vectorizing and storing the document chunks in a vector store for similarity search\n",
    "> - Setting up a retriever to find relevant sections based on a userâ€™s query\n",
    "> - Integrating with OpenAI to generate contextually relevant responses based on the retrieved information\n",
    "\n",
    "\n",
    "\n",
    "# ðŸ”— References:\n",
    "- [LangChain - PDF Q/A](https://python.langchain.com/docs/tutorials/pdf_qa/)\n",
    "- [LangChain Tutorials](https://python.langchain.com/docs/tutorials/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
